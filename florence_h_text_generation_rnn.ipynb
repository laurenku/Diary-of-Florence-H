{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurenku/Diary-of-Florence-H/blob/main/florence_h_text_generation_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL3ZPJPgC4wC"
      },
      "source": [
        "# Shakespeare Text Generation (using RNN LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoWGtkL8PPJ3"
      },
      "source": [
        "> - ðŸ¤– See [full list of Machine Learning Experiments](https://github.com/trekhleb/machine-learning-experiments) on **GitHub**<br/><br/>\n",
        "> - â–¶ï¸ **Interactive Demo**: [try this model and other machine learning experiments in action](https://trekhleb.github.io/machine-learning-experiments/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bui0MyTjv1Mp"
      },
      "source": [
        "## Experiment overview\n",
        "\n",
        "In this experiment we will use character-based [Recurrent Neural Network](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNN) to generate a Shakespeare's-like text based on the Shakespeare dataset from [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) blog post.\n",
        "\n",
        "For this experiment we will use [Tensorflow v2](https://www.tensorflow.org/) with its [Keras API](https://www.tensorflow.org/guide/keras).\n",
        "\n",
        "![text_generation_shakespeare_rnn.jpg](https://github.com/trekhleb/machine-learning-experiments/blob/master/demos/src/images/text_generation_shakespeare_rnn.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XxEZuRNIzHH"
      },
      "source": [
        "_Inspired by [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDJctDhhEDTD"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2xZnxncD2Ep"
      },
      "outputs": [],
      "source": [
        "# Selecting Tensorflow version v2 (the command is relevant for Colab only).\n",
        "# %tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpueB6zADYgE",
        "outputId": "ab3e6cbb-1d5d-43bf-909d-5ebfc7c860ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 572 kB 31.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.0 MB 53.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87 kB 7.0 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 584 kB 53.2 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 409 kB 57.3 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 136 kB 65.8 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 9.9 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1 MB 51.1 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 880 kB 49.2 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 56.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77 kB 4.7 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 94 kB 3.1 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 271 kB 72.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144 kB 77.8 MB/s \n",
            "\u001b[?25h  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Python version: 3.7.13\n",
            "Tensorflow version: 2.8.0\n",
            "Keras version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import platform\n",
        "import time\n",
        "import pathlib\n",
        "import os\n",
        "\n",
        "!pip install -q aitextgen\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s â€” %(levelname)s â€” %(name)s â€” %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO\n",
        "    )\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive\n",
        "\n",
        "print('Python version:', platform.python_version())\n",
        "print('Tensorflow version:', tf.__version__)\n",
        "print('Keras version:', tf.keras.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciI_JnnNEGCw"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H8uIfirQKXB",
        "outputId": "8def0625-8d96-4d99-a4ab-593bb5dc6e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPtmrHEut-nn",
        "outputId": "86337e01-7593-44e2-ef4e-2dca70e0859f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "mount_gdrive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtFjDI34v9Dk"
      },
      "outputs": [],
      "source": [
        "letter_dataset_file_path = \"/content/drive/MyDrive/sarah_letters_dataset.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OHMJp5NgBtD"
      },
      "outputs": [],
      "source": [
        "email_dataset_file_path = \"/content/drive/MyDrive/diary_dataset.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKTy6YS5Gx-g"
      },
      "source": [
        "## Analyze the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzfX7hK8G0bn",
        "outputId": "42f2777c-6b05-4766-bca2-672d1eb197d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 157549 characters\n"
          ]
        }
      ],
      "source": [
        "# Reading the database file.\n",
        "letter_text = open(letter_dataset_file_path, mode='r').read()\n",
        "\n",
        "print('Length of text: {} characters'.format(len(letter_text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VpC4pXygG1j",
        "outputId": "3f0d13e7-fc09-43a3-88b3-7eb262fc991b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 41015 characters\n"
          ]
        }
      ],
      "source": [
        "email_text = open(email_dataset_file_path, mode='r').read()\n",
        "\n",
        "print('Length of text: {} characters'.format(len(email_text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nknOAOuoH-Av"
      },
      "outputs": [],
      "source": [
        "# Take a look at the first 250 characters in text.\n",
        "# print(letter_text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPXkB5bOITqi",
        "outputId": "02e377a7-78f0-4054-fbdc-2348ac72ff42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91 unique characters\n",
            "vocab: ['\\t', '\\n', ' ', '!', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ã©', 'â€™', 'â€œ', 'â€', 'â€¦', 'â…“', 'â…”', 'â…˜']\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "letter_vocab = sorted(set(letter_text))\n",
        "\n",
        "print('{} unique characters'.format(len(letter_vocab)))\n",
        "print('vocab:', letter_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEeTORZJgSOa",
        "outputId": "9e149c68-291f-4702-bd1b-6e7afbbcac94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91 unique characters\n",
            "vocab: ['\\n', ' ', '!', '\"', '#', '%', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ã±', 'Ðµ', 'â€“', 'â€™', 'â€œ', 'â€', 'â€¦']\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "email_vocab = sorted(set(email_text))\n",
        "\n",
        "print('{} unique characters'.format(len(email_vocab)))\n",
        "print('vocab:', email_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqpuKh9HMNnf"
      },
      "source": [
        "## Process the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dj4e-AGMaV4"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before feeding the text to our RNN we need to convert the text from a sequence of characters to a sequence of numbers. To do so we will detect all unique characters in the text, form a vocabulary out of it and replace each character with its index in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFFpuXfGMPq2",
        "outputId": "258b93a9-989a-4f78-ea79-1222917770ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  '\\t':   0,\n",
            "  '\\n':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '#' :   4,\n",
            "  '$' :   5,\n",
            "  '%' :   6,\n",
            "  '&' :   7,\n",
            "  \"'\" :   8,\n",
            "  '(' :   9,\n",
            "  ')' :  10,\n",
            "  '*' :  11,\n",
            "  '+' :  12,\n",
            "  ',' :  13,\n",
            "  '-' :  14,\n",
            "  '.' :  15,\n",
            "  '/' :  16,\n",
            "  '0' :  17,\n",
            "  '1' :  18,\n",
            "  '2' :  19,\n",
            "  ...\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Map characters to their indices in vocabulary.\n",
        "letter_char2index = {char: index for index, char in enumerate(letter_vocab)}\n",
        "\n",
        "print('{')\n",
        "for char, _ in zip(letter_char2index, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), letter_char2index[char]))\n",
        "print('  ...\\n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyXj0tD1geyR",
        "outputId": "1c86471b-957b-4e94-d0df-41c94e4a0ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '\"' :   3,\n",
            "  '#' :   4,\n",
            "  '%' :   5,\n",
            "  \"'\" :   6,\n",
            "  '(' :   7,\n",
            "  ')' :   8,\n",
            "  '*' :   9,\n",
            "  '+' :  10,\n",
            "  ',' :  11,\n",
            "  '-' :  12,\n",
            "  '.' :  13,\n",
            "  '/' :  14,\n",
            "  '0' :  15,\n",
            "  '1' :  16,\n",
            "  '2' :  17,\n",
            "  '3' :  18,\n",
            "  '4' :  19,\n",
            "  ...\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Map characters to their indices in vocabulary.\n",
        "email_char2index = {char: index for index, char in enumerate(email_vocab)}\n",
        "\n",
        "print('{')\n",
        "for char, _ in zip(email_char2index, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), email_char2index[char]))\n",
        "print('  ...\\n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sb6hF1Jgn89",
        "outputId": "d7960f2e-3108-4e06-9de0-23855fdd6100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\t' '\\n' ' ' '!' '#' '$' '%' '&' \"'\" '(' ')' '*' '+' ',' '-' '.' '/' '0'\n",
            " '1' '2' '3' '4' '5' '6' '7' '8' '9' ':' ';' '=' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' 'Ã©' 'â€™' 'â€œ' 'â€' 'â€¦' 'â…“' 'â…”'\n",
            " 'â…˜']\n"
          ]
        }
      ],
      "source": [
        "# Map character indices to characters from vacabulary.\n",
        "letter_index2char = np.array(letter_vocab)\n",
        "print(letter_index2char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5rviiSWgwzn",
        "outputId": "74c2e840-08fb-4f75-ecf9-cb639467e51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n' ' ' '!' '\"' '#' '%' \"'\" '(' ')' '*' '+' ',' '-' '.' '/' '0' '1' '2'\n",
            " '3' '4' '5' '6' '7' '8' '9' ':' ';' '=' '?' '@' 'A' 'B' 'C' 'D' 'E' 'F'\n",
            " 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'Y'\n",
            " 'Z' '[' ']' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n'\n",
            " 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' 'Ã±' 'Ðµ' 'â€“' 'â€™' 'â€œ' 'â€'\n",
            " 'â€¦']\n"
          ]
        }
      ],
      "source": [
        "# Map character indices to characters from vacabulary.\n",
        "email_index2char = np.array(email_vocab)\n",
        "print(email_index2char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXUAlYmvN_Rj",
        "outputId": "b9e29556-3bd8-4d3f-f49f-8509afefa2aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_as_int length: 157549\n",
            "'Dear Lauren,\\nI ' --> array([34, 61, 57, 74,  2, 42, 57, 77, 74, 61, 70, 13,  1, 39,  2])\n"
          ]
        }
      ],
      "source": [
        "# Convert chars in text to indices.\n",
        "letter_text_as_int = np.array([letter_char2index[char] for char in letter_text])\n",
        "\n",
        "print('text_as_int length: {}'.format(len(letter_text_as_int)))\n",
        "print('{} --> {}'.format(repr(letter_text[:15]), repr(letter_text_as_int[:15])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdjxwW5fg5Xj",
        "outputId": "acfef448-b146-4754-b013-abf380d52855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_as_int length: 41015\n",
            "'Monday, Novembe' --> array([42, 72, 71, 61, 58, 82, 11,  1, 43, 72, 79, 62, 70, 59, 62])\n"
          ]
        }
      ],
      "source": [
        "# Convert chars in text to indices.\n",
        "email_text_as_int = np.array([email_char2index[char] for char in email_text])\n",
        "\n",
        "print('text_as_int length: {}'.format(len(email_text_as_int)))\n",
        "print('{} --> {}'.format(repr(email_text[:15]), repr(email_text_as_int[:15])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHv5HhUuTQYS"
      },
      "source": [
        "## Create training sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tHT8IXGTTtZ"
      },
      "outputs": [],
      "source": [
        "# The maximum length sentence we want for a single input in characters.\n",
        "sequence_length = 100\n",
        "# examples_per_epoch = len(letter_text) // (sequence_length + 1)\n",
        "\n",
        "# print('examples_per_epoch:', examples_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf5tMMDnUP3q",
        "outputId": "f81fa5da-c9ef-42d9-b9f2-5a18e8c66827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D\n",
            "e\n",
            "a\n",
            "r\n",
            " \n"
          ]
        }
      ],
      "source": [
        "# Create training dataset.\n",
        "letter_char_dataset = tf.data.Dataset.from_tensor_slices(letter_text_as_int)\n",
        "\n",
        "for char in letter_char_dataset.take(5):\n",
        "    print(letter_index2char[char.numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD0lM-KJhZ9L",
        "outputId": "29d633e6-2c0d-49b2-8e57-77aeaf38e7b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M\n",
            "o\n",
            "n\n",
            "d\n",
            "a\n"
          ]
        }
      ],
      "source": [
        "email_char_dataset = tf.data.Dataset.from_tensor_slices(email_text_as_int)\n",
        "\n",
        "for char in email_char_dataset.take(5):\n",
        "    print(email_index2char[char.numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap71VjB2Vuct"
      },
      "outputs": [],
      "source": [
        "# Generate batched sequences out of the char_dataset.\n",
        "letter_sequences = letter_char_dataset.batch(sequence_length + 1, drop_remainder=True)\n",
        "\n",
        "# Sequences size is the same as examples_per_epoch.\n",
        "# print('Sequences count: {}'.format(len(list(sequences.as_numpy_iterator()))));\n",
        "# print()\n",
        "\n",
        "# Sequences examples.\n",
        "# for item in sequences.take(5):\n",
        "#     print(repr(''.join(letter_index2char[item.numpy()])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlDu-nhchgUU"
      },
      "outputs": [],
      "source": [
        "email_sequences = email_char_dataset.batch(sequence_length + 1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8spPCfe-iTn"
      },
      "outputs": [],
      "source": [
        "# sequences shape:\n",
        "# - 11043 sequences\n",
        "# - Each sequence of length 101\n",
        "#\n",
        "#\n",
        "#    101     101          101\n",
        "# [(.....) (.....) ...  (.....)]\n",
        "#\n",
        "# <---------- 11043 ----------->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdcrcUs4Xxso"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text. For example, say `sequence_length` is `4` and our text is `Hello`. The input sequence would be `Hell`, and the target sequence `ello`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fxvXsP0XFDh"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "454rWIQYXXRY"
      },
      "outputs": [],
      "source": [
        "letter_dataset = letter_sequences.map(split_input_target)\n",
        "\n",
        "# Dataset size is the same as examples_per_epoch.\n",
        "# But each element of a sequence is now has length of `sequence_length`\n",
        "# and not `sequence_length + 1`.\n",
        "# print('dataset size: {}'.format(len(list(dataset.as_numpy_iterator()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAoMQem1hjvG"
      },
      "outputs": [],
      "source": [
        "email_dataset = email_sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuoh4tCdYCck"
      },
      "outputs": [],
      "source": [
        "# for input_example, target_example in dataset.take(1):\n",
        "#     print('Input sequence size:', repr(len(input_example.numpy())))\n",
        "#     print('Target sequence size:', repr(len(target_example.numpy())))\n",
        "#     print()\n",
        "#     print('Input:', repr(''.join(index2char[input_example.numpy()])))\n",
        "#     print('Target:', repr(''.join(index2char[target_example.numpy()])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp0tl0sN807l"
      },
      "outputs": [],
      "source": [
        "# dataset shape:\n",
        "# - 11043 sequences\n",
        "# - Each sequence is a tuple of 2 sub-sequences of length 100 (input_text and target_text)\n",
        "#\n",
        "#\n",
        "#    100       100           100\n",
        "# /(.....)\\ /(.....)\\ ... /(.....)\\  <-- input_text\n",
        "# \\(.....)/ \\(.....)/     \\(.....)/  <-- target_text\n",
        "#\n",
        "# <----------- 11043 ------------->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDYHEJ0pY1ai"
      },
      "source": [
        "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the RNN considers the previous step context in addition to the current input character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-0zpv53Y2o4"
      },
      "outputs": [],
      "source": [
        "# for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "#     print('Step {:2d}'.format(i))\n",
        "#     print('  input: {} ({:s})'.format(input_idx, repr(index2char[input_idx])))\n",
        "#     print('  expected output: {} ({:s})'.format(target_idx, repr(index2char[target_idx])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iDlp40lC5YB"
      },
      "source": [
        "## Split training sequences into batches\n",
        "\n",
        "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDq-wa5EC3wW",
        "outputId": "0027180c-c9ee-466d-ff8a-5f127de18731"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Batch size.\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset (TF data is designed to work\n",
        "# with possibly infinite sequences, so it doesn't attempt to shuffle\n",
        "# the entire sequence in memory. Instead, it maintains a buffer in\n",
        "# which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "letter_dataset = letter_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "letter_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_n1aIrJhowZ",
        "outputId": "ef56faac-a49d-4ad3-b76a-4cf4675daa4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "email_dataset = email_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "email_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x4puZiiOlyl",
        "outputId": "2d26a3cb-55d3-432e-fe39-66e0da7d8f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batched dataset size: 24\n"
          ]
        }
      ],
      "source": [
        "print('Batched dataset size: {}'.format(len(list(letter_dataset.as_numpy_iterator()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_kYgvQGBO0U",
        "outputId": "d77324ec-497e-4efc-ab0b-136219d4e4fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1st batch: input_text: tf.Tensor(\n",
            "[[61 60 60 ... 74  2 31]\n",
            " [74 61 80 ... 63 71 76]\n",
            " [74  2 76 ... 65 70 70]\n",
            " ...\n",
            " [74  2 59 ... 74 65 75]\n",
            " [ 2 65 76 ... 65 68 81]\n",
            " [72  2 76 ... 76 65 59]], shape=(64, 100), dtype=int64)\n",
            "\n",
            "1st batch: target_text: tf.Tensor(\n",
            "[[60 60 65 ...  2 31 70]\n",
            " [61 80 30 ... 71 76  2]\n",
            " [ 2 76 64 ... 70 70 57]\n",
            " ...\n",
            " [ 2 59 57 ... 65 75 65]\n",
            " [65 76  2 ... 68 81  2]\n",
            " [ 2 76 64 ... 65 59 77]], shape=(64, 100), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for input_text, target_text in letter_dataset.take(1):\n",
        "    print('1st batch: input_text:', input_text)\n",
        "    print()\n",
        "    print('1st batch: target_text:', target_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkDCH15v_2I6"
      },
      "outputs": [],
      "source": [
        "# dataset shape:\n",
        "# - 172 batches\n",
        "# - 64 sequences per batch\n",
        "# - Each sequence is a tuple of 2 sub-sequences of length 100 (input_text and target_text)\n",
        "#\n",
        "#\n",
        "#     100       100           100             100       100           100\n",
        "# |/(.....)\\ /(.....)\\ ... /(.....)\\| ... |/(.....)\\ /(.....)\\ ... /(.....)\\|  <-- input_text\n",
        "# |\\(.....)/ \\(.....)/     \\(.....)/| ... |\\(.....)/ \\(.....)/     \\(.....)/|  <-- target_text\n",
        "#\n",
        "# <------------- 64 ---------------->     <------------- 64 ---------------->\n",
        "#\n",
        "# <--------------------------------- 172 ----------------------------------->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghB-VwLlD-Oz"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Use [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "- [tf.keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding): The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
        "- [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): A type of RNN with size units=rnn_units (You can also use a GRU layer here.)\n",
        "- [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): The output layer, with vocab_size outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cg8DlO3QjuT",
        "outputId": "c8e6fde8-ea77-417e-ba62-5705c795cbac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tmp_input_array shape: (2, 8)\n",
            "tmp_input_array:\n",
            "[[2 7 0 4 9 0 0 4]\n",
            " [9 7 8 4 5 6 1 1]]\n",
            "\n",
            "tmp_output_array shape: (2, 8, 5)\n",
            "tmp_output_array:\n",
            "[[[-0.01200955  0.01147382  0.00872859 -0.02693832 -0.02201879]\n",
            "  [-0.04144623 -0.00375894  0.04627063  0.03208634  0.02370707]\n",
            "  [ 0.01249873  0.03637045  0.04640576  0.00632981  0.02358804]\n",
            "  [ 0.04743221 -0.04089658 -0.01265769 -0.02695099  0.02311535]\n",
            "  [ 0.04808911 -0.04659972  0.02610648 -0.01634566 -0.0387959 ]\n",
            "  [ 0.01249873  0.03637045  0.04640576  0.00632981  0.02358804]\n",
            "  [ 0.01249873  0.03637045  0.04640576  0.00632981  0.02358804]\n",
            "  [ 0.04743221 -0.04089658 -0.01265769 -0.02695099  0.02311535]]\n",
            "\n",
            " [[ 0.04808911 -0.04659972  0.02610648 -0.01634566 -0.0387959 ]\n",
            "  [-0.04144623 -0.00375894  0.04627063  0.03208634  0.02370707]\n",
            "  [-0.0435761  -0.016555   -0.03373598  0.0196736  -0.04830784]\n",
            "  [ 0.04743221 -0.04089658 -0.01265769 -0.02695099  0.02311535]\n",
            "  [-0.04524432  0.0004215  -0.00537046 -0.01894831 -0.00221858]\n",
            "  [ 0.0227293  -0.04350249  0.00113519  0.01125696  0.00325506]\n",
            "  [-0.01941136  0.02125431 -0.03765984 -0.03593232  0.01099353]\n",
            "  [-0.01941136  0.02125431 -0.03765984 -0.03593232  0.01099353]]]\n"
          ]
        }
      ],
      "source": [
        "# Let's do a quick detour and see how Embeding layer works.\n",
        "# It takes several char indices sequences (batch) as an input.\n",
        "# It encodes every character of every sequence to a vector of tmp_embeding_size length.\n",
        "tmp_vocab_size = 10\n",
        "tmp_embeding_size = 5\n",
        "tmp_input_length = 8\n",
        "tmp_batch_size = 2\n",
        "\n",
        "tmp_model = tf.keras.models.Sequential()\n",
        "tmp_model.add(tf.keras.layers.Embedding(\n",
        "  input_dim=tmp_vocab_size,\n",
        "  output_dim=tmp_embeding_size,\n",
        "  input_length=tmp_input_length\n",
        "))\n",
        "# The model will take as input an integer matrix of size (batch, input_length).\n",
        "# The largest integer (i.e. word index) in the input should be no larger than 9 (tmp_vocab_size).\n",
        "# Now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
        "tmp_input_array = np.random.randint(\n",
        "  low=0,\n",
        "  high=tmp_vocab_size,\n",
        "  size=(tmp_batch_size, tmp_input_length)\n",
        ")\n",
        "tmp_model.compile('rmsprop', 'mse')\n",
        "tmp_output_array = tmp_model.predict(tmp_input_array)\n",
        "\n",
        "print('tmp_input_array shape:', tmp_input_array.shape)\n",
        "print('tmp_input_array:')\n",
        "print(tmp_input_array)\n",
        "print()\n",
        "print('tmp_output_array shape:', tmp_output_array.shape)\n",
        "print('tmp_output_array:')\n",
        "print(tmp_output_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7ZuvZHBD_pS"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars.\n",
        "vocab_size = len(letter_vocab)\n",
        "\n",
        "# The embedding dimension.\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units.\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sojdDCAICWO"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.models.Sequential()\n",
        "\n",
        "    model.add(tf.keras.layers.Embedding(\n",
        "      input_dim=vocab_size,\n",
        "      output_dim=embedding_dim,\n",
        "      batch_input_shape=[batch_size, None]\n",
        "    ))\n",
        "\n",
        "    model.add(tf.keras.layers.LSTM(\n",
        "      units=rnn_units,\n",
        "      return_sequences=True,\n",
        "      stateful=True,\n",
        "      recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
        "    ))\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(vocab_size))\n",
        "  \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoPwxyAPEg6z"
      },
      "outputs": [],
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLnlZFgU55bQ",
        "outputId": "4fc21ee9-940c-417b-e21e-98fa55036dd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           23296     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 91)            93275     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,363,547\n",
            "Trainable params: 5,363,547\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "CcaO_rO_8-GH",
        "outputId": "795fd3f2-8067-4435-e604-9aa6c4c578e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpL9idwZV6fL"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![Model architecture](https://www.tensorflow.org/tutorials/text/images/text_generation_training.png)\n",
        "\n",
        "Image source: [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation) notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npruiy2RAPkt"
      },
      "source": [
        "## Try the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4DCLA0GASL1",
        "outputId": "8ac51eaf-aa3f-4cd9-ae8a-ea8eb33247bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 91) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in letter_dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWebJXU9CEPd"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4Jgo-iECFWI",
        "outputId": "846de576-f821-413a-93f8-15c5b578e8ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for the 1st letter of the batch 1st sequense:\n",
            "tf.Tensor(\n",
            "[ 2.8644362e-04 -1.6578366e-03 -4.0072082e-03 -2.5744841e-03\n",
            " -2.7736272e-03  5.1083090e-03  3.3059637e-03  6.5479637e-04\n",
            "  5.2425722e-03 -2.1601976e-03  2.5684994e-03  6.5270364e-03\n",
            "  2.1240534e-03  4.1304021e-03  6.7108255e-03  7.4530224e-05\n",
            "  7.7067583e-04 -2.0353328e-03  1.3633503e-03 -2.5814064e-03\n",
            "  1.2892294e-03  3.8025994e-04  3.6732531e-03  2.5010274e-03\n",
            "  9.7803399e-04  2.3212666e-03 -5.3074094e-03 -1.2276797e-03\n",
            " -7.1262522e-04  4.4715893e-04  4.0174887e-04  1.7219437e-03\n",
            " -4.2954884e-03  1.1311976e-03  1.8147063e-03 -4.2372774e-03\n",
            "  1.3581873e-03  1.1376891e-04  6.1154086e-03 -8.4739283e-04\n",
            "  4.7271871e-03  4.0987013e-03 -3.8382348e-03 -2.0586832e-03\n",
            " -2.7358569e-03  2.2140536e-03 -1.3665791e-03 -1.4555526e-03\n",
            "  8.1459191e-03 -1.5349687e-03  4.8038503e-03 -1.1402699e-03\n",
            "  4.2874534e-03 -4.3328718e-04 -3.0297739e-04  3.1036600e-03\n",
            "  4.4902484e-03  6.8493406e-03  4.2649391e-03 -5.3357920e-03\n",
            " -2.1151050e-03  2.3028546e-03 -2.0582217e-03  1.9339706e-03\n",
            "  1.8746746e-03 -1.0560229e-03  7.7850418e-05  3.6466080e-03\n",
            "  4.4472963e-03  9.0063475e-03 -2.4928888e-03  7.4248493e-04\n",
            "  4.8664715e-03  2.7880862e-03  4.0158518e-03  2.6855448e-03\n",
            " -2.3534722e-03  6.5967627e-03 -7.3368021e-04 -2.8844061e-04\n",
            " -2.3532375e-03  1.1716939e-03  5.4205493e-03 -4.0242537e-03\n",
            " -5.1947045e-03  2.0618162e-03 -3.1099708e-03 -2.7828363e-03\n",
            "  7.2709646e-04 -1.9042564e-03 -1.7877310e-03], shape=(91,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print('Prediction for the 1st letter of the batch 1st sequense:')\n",
        "print(example_batch_predictions[0, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dOr0MwFHlRb",
        "outputId": "71467317-2c2f-40e9-af00-f603e60e862e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[2 2 1 1 2]], shape=(1, 5), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# Quick overview of how tf.random.categorical() works.\n",
        "\n",
        "# logits is 2-D Tensor with shape [batch_size, num_classes].\n",
        "# Each slice [i, :] represents the unnormalized log-probabilities for all classes.\n",
        "# In the example below we say that the probability for class \"0\" is low but the\n",
        "# probability for class \"2\" is much higher.\n",
        "tmp_logits = [\n",
        "  [-0.95, 0, 0.95],\n",
        "];\n",
        "\n",
        "# Let's generate 5 samples. Each sample is a class index. Class probabilities \n",
        "# are being taken into account (we expect to see more samples of class \"2\").\n",
        "tmp_samples = tf.random.categorical(\n",
        "    logits=tmp_logits,\n",
        "    num_samples=5\n",
        ")\n",
        "\n",
        "print(tmp_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPzr0r4zCgS3",
        "outputId": "1f8c9c9a-4dda-441a-897d-db1538736f86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([100, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    logits=example_batch_predictions[0],\n",
        "    num_samples=1\n",
        ")\n",
        "\n",
        "sampled_indices.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaA7DclID8dz",
        "outputId": "d8940d54-97ad-4d8b-d73b-f71e2999b9ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "sampled_indices = tf.squeeze(\n",
        "    input=sampled_indices,\n",
        "    axis=-1\n",
        ").numpy()\n",
        "\n",
        "sampled_indices.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ubGQ0gVENhB",
        "outputId": "d3062c07-b19f-404c-821e-ffa9f33259dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([31, 17, 54, 55, 33, 68, 40, 29, 10, 88,  5, 79, 82, 58, 36, 64, 60,\n",
              "       14, 27, 30, 66, 85, 49, 39, 40, 67, 47, 29, 30, 34, 54, 47,  0, 23,\n",
              "       51, 10, 62, 29, 50, 75, 11, 22, 51, 17, 70, 79, 59, 46, 77, 40,  0,\n",
              "       72, 16, 37, 12, 79, 38, 74, 43, 75, 75, 42, 89, 37,  9,  9, 36, 14,\n",
              "       52,  0, 77, 51,  6, 51, 15, 85, 27,  8, 72, 25, 69, 36,  2, 31, 52,\n",
              "       88, 24,  4, 49, 74, 31, 44,  9, 25, 42, 34,  7,  6, 38, 47])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi9HOzw9EajS"
      },
      "outputs": [],
      "source": [
        "# print('Input:\\n', repr(''.join(index2char[input_example_batch[0]])))\n",
        "# print()\n",
        "# print('Next char prediction:\\n', repr(''.join(index2char[sampled_indices])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b87e0lsYMTsv"
      },
      "outputs": [],
      "source": [
        "# for i, (input_idx, sample_idx) in enumerate(zip(input_example_batch[0][:5], sampled_indices[:5])):\n",
        "#     print('Prediction {:2d}'.format(i))\n",
        "#     print('  input: {} ({:s})'.format(input_idx, repr(index2char[input_idx])))\n",
        "#     print('  next predicted: {} ({:s})'.format(target_idx, repr(index2char[sample_idx])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqcBufKEE_p6"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4s0-PvrFub5"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOEUUm6JE95a",
        "outputId": "44d6d6a9-6473-47cc-d0a9-73c63fd60eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 91)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.509608\n"
          ]
        }
      ],
      "source": [
        "# An objective function.\n",
        "# The function is any callable with the signature scalar_loss = fn(y_true, y_pred).\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
        "      y_true=labels,\n",
        "      y_pred=logits,\n",
        "      from_logits=True\n",
        "    )\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXhJsB6eFgrJ"
      },
      "outputs": [],
      "source": [
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss=loss\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK3Cf-xZFwL4"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUhXnHPJFy5q"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved.\n",
        "checkpoint_dir = 'tmp/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFg9MFJoGZWf"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVk-pARPGaja"
      },
      "outputs": [],
      "source": [
        "EPOCHS=1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0rveBdAGeEz",
        "outputId": "2451ea9f-08e4-4695-c0ee-91071a5ec052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "6/6 [==============================] - 4s 100ms/step - loss: 3.8915\n",
            "Epoch 2/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 3.2404\n",
            "Epoch 3/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 3.2025\n",
            "Epoch 4/1000\n",
            "6/6 [==============================] - 1s 81ms/step - loss: 3.1732\n",
            "Epoch 5/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 3.1396\n",
            "Epoch 6/1000\n",
            "6/6 [==============================] - 1s 80ms/step - loss: 3.0972\n",
            "Epoch 7/1000\n",
            "6/6 [==============================] - 1s 81ms/step - loss: 3.0240\n",
            "Epoch 8/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 2.9198\n",
            "Epoch 9/1000\n",
            "6/6 [==============================] - 1s 80ms/step - loss: 2.8044\n",
            "Epoch 10/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 2.7001\n",
            "Epoch 11/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 2.6282\n",
            "Epoch 12/1000\n",
            "6/6 [==============================] - 1s 82ms/step - loss: 2.8895\n",
            "Epoch 13/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 2.5694\n",
            "Epoch 14/1000\n",
            "6/6 [==============================] - 1s 82ms/step - loss: 2.5187\n",
            "Epoch 15/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 2.4715\n",
            "Epoch 16/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 2.4310\n",
            "Epoch 17/1000\n",
            "6/6 [==============================] - 1s 79ms/step - loss: 2.3808\n",
            "Epoch 18/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 2.3433\n",
            "Epoch 19/1000\n",
            "6/6 [==============================] - 1s 81ms/step - loss: 2.3048\n",
            "Epoch 20/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 2.2649\n",
            "Epoch 21/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 2.2254\n",
            "Epoch 22/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 2.1798\n",
            "Epoch 23/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 2.1418\n",
            "Epoch 24/1000\n",
            "6/6 [==============================] - 1s 81ms/step - loss: 2.1027\n",
            "Epoch 25/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 2.0621\n",
            "Epoch 26/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 2.0336\n",
            "Epoch 27/1000\n",
            "6/6 [==============================] - 1s 82ms/step - loss: 1.9838\n",
            "Epoch 28/1000\n",
            "6/6 [==============================] - 1s 80ms/step - loss: 1.9453\n",
            "Epoch 29/1000\n",
            "6/6 [==============================] - 1s 80ms/step - loss: 1.9097\n",
            "Epoch 30/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 1.8763\n",
            "Epoch 31/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 1.8354\n",
            "Epoch 32/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 1.7995\n",
            "Epoch 33/1000\n",
            "6/6 [==============================] - 1s 82ms/step - loss: 1.7674\n",
            "Epoch 34/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 1.7306\n",
            "Epoch 35/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 1.6888\n",
            "Epoch 36/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 1.6581\n",
            "Epoch 37/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 1.6135\n",
            "Epoch 38/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 1.5750\n",
            "Epoch 39/1000\n",
            "6/6 [==============================] - 1s 80ms/step - loss: 1.5505\n",
            "Epoch 40/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 1.5112\n",
            "Epoch 41/1000\n",
            "6/6 [==============================] - 1s 81ms/step - loss: 1.4731\n",
            "Epoch 42/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 1.4379\n",
            "Epoch 43/1000\n",
            "6/6 [==============================] - 1s 81ms/step - loss: 1.3897\n",
            "Epoch 44/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 1.3568\n",
            "Epoch 45/1000\n",
            "6/6 [==============================] - 1s 163ms/step - loss: 1.3144\n",
            "Epoch 46/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 1.2772\n",
            "Epoch 47/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 1.2399\n",
            "Epoch 48/1000\n",
            "6/6 [==============================] - 1s 81ms/step - loss: 1.2118\n",
            "Epoch 49/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 1.1791\n",
            "Epoch 50/1000\n",
            "6/6 [==============================] - 1s 82ms/step - loss: 1.1289\n",
            "Epoch 51/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 1.0914\n",
            "Epoch 52/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 1.0515\n",
            "Epoch 53/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 1.0130\n",
            "Epoch 54/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.9756\n",
            "Epoch 55/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.9328\n",
            "Epoch 56/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.9040\n",
            "Epoch 57/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 0.8657\n",
            "Epoch 58/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.8185\n",
            "Epoch 59/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.7763\n",
            "Epoch 60/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.7448\n",
            "Epoch 61/1000\n",
            "6/6 [==============================] - 1s 156ms/step - loss: 0.7036\n",
            "Epoch 62/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.6723\n",
            "Epoch 63/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.6395\n",
            "Epoch 64/1000\n",
            "6/6 [==============================] - 1s 81ms/step - loss: 0.6094\n",
            "Epoch 65/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.5781\n",
            "Epoch 66/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.5589\n",
            "Epoch 67/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.5237\n",
            "Epoch 68/1000\n",
            "6/6 [==============================] - 1s 82ms/step - loss: 0.4928\n",
            "Epoch 69/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.4632\n",
            "Epoch 70/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.4468\n",
            "Epoch 71/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.4231\n",
            "Epoch 72/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.4026\n",
            "Epoch 73/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.3852\n",
            "Epoch 74/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.3750\n",
            "Epoch 75/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 0.3587\n",
            "Epoch 76/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.3431\n",
            "Epoch 77/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.3266\n",
            "Epoch 78/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.3161\n",
            "Epoch 79/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.3120\n",
            "Epoch 80/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.3053\n",
            "Epoch 81/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.2920\n",
            "Epoch 82/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.2829\n",
            "Epoch 83/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.2751\n",
            "Epoch 84/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.2747\n",
            "Epoch 85/1000\n",
            "6/6 [==============================] - 1s 81ms/step - loss: 0.2634\n",
            "Epoch 86/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.2569\n",
            "Epoch 87/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.2531\n",
            "Epoch 88/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.2503\n",
            "Epoch 89/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.2388\n",
            "Epoch 90/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.2402\n",
            "Epoch 91/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 0.2329\n",
            "Epoch 92/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.2241\n",
            "Epoch 93/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.2231\n",
            "Epoch 94/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.2215\n",
            "Epoch 95/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 0.2216\n",
            "Epoch 96/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.2122\n",
            "Epoch 97/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.2088\n",
            "Epoch 98/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.2078\n",
            "Epoch 99/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.2052\n",
            "Epoch 100/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.2029\n",
            "Epoch 101/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.2019\n",
            "Epoch 102/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1913\n",
            "Epoch 103/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.1973\n",
            "Epoch 104/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1944\n",
            "Epoch 105/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.1906\n",
            "Epoch 106/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.1827\n",
            "Epoch 107/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.1820\n",
            "Epoch 108/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.1801\n",
            "Epoch 109/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1818\n",
            "Epoch 110/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.1731\n",
            "Epoch 111/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 0.1771\n",
            "Epoch 112/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.1740\n",
            "Epoch 113/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1743\n",
            "Epoch 114/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.1702\n",
            "Epoch 115/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.1689\n",
            "Epoch 116/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1667\n",
            "Epoch 117/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.1651\n",
            "Epoch 118/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1652\n",
            "Epoch 119/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.1638\n",
            "Epoch 120/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1625\n",
            "Epoch 121/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.1563\n",
            "Epoch 122/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.1598\n",
            "Epoch 123/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.1558\n",
            "Epoch 124/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1543\n",
            "Epoch 125/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.1532\n",
            "Epoch 126/1000\n",
            "6/6 [==============================] - 1s 82ms/step - loss: 0.1557\n",
            "Epoch 127/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.1504\n",
            "Epoch 128/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1508\n",
            "Epoch 129/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.1480\n",
            "Epoch 130/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.1479\n",
            "Epoch 131/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.1436\n",
            "Epoch 132/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 0.1454\n",
            "Epoch 133/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.1415\n",
            "Epoch 134/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1428\n",
            "Epoch 135/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.1447\n",
            "Epoch 136/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1367\n",
            "Epoch 137/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.1379\n",
            "Epoch 138/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1394\n",
            "Epoch 139/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.1373\n",
            "Epoch 140/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1409\n",
            "Epoch 141/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.1376\n",
            "Epoch 142/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1344\n",
            "Epoch 143/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.1321\n",
            "Epoch 144/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1310\n",
            "Epoch 145/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.1369\n",
            "Epoch 146/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1295\n",
            "Epoch 147/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.1300\n",
            "Epoch 148/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.1311\n",
            "Epoch 149/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1295\n",
            "Epoch 150/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.1307\n",
            "Epoch 151/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1272\n",
            "Epoch 152/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.1270\n",
            "Epoch 153/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1249\n",
            "Epoch 154/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.1272\n",
            "Epoch 155/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.1203\n",
            "Epoch 156/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1220\n",
            "Epoch 157/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.1215\n",
            "Epoch 158/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1210\n",
            "Epoch 159/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.1190\n",
            "Epoch 160/1000\n",
            "6/6 [==============================] - 1s 131ms/step - loss: 0.1215\n",
            "Epoch 161/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.1167\n",
            "Epoch 162/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.1149\n",
            "Epoch 163/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.1205\n",
            "Epoch 164/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.1157\n",
            "Epoch 165/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.1161\n",
            "Epoch 166/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1161\n",
            "Epoch 167/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.1129\n",
            "Epoch 168/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1136\n",
            "Epoch 169/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.1138\n",
            "Epoch 170/1000\n",
            "6/6 [==============================] - 1s 84ms/step - loss: 0.1118\n",
            "Epoch 171/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.1104\n",
            "Epoch 172/1000\n",
            "6/6 [==============================] - 1s 83ms/step - loss: 0.1127\n",
            "Epoch 173/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.1108\n",
            "Epoch 174/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1101\n",
            "Epoch 175/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.1086\n",
            "Epoch 176/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1080\n",
            "Epoch 177/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.1087\n",
            "Epoch 178/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.1085\n",
            "Epoch 179/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.1071\n",
            "Epoch 180/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.1111\n",
            "Epoch 181/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.1041\n",
            "Epoch 182/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.1069\n",
            "Epoch 183/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.1040\n",
            "Epoch 184/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.1016\n",
            "Epoch 185/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.1029\n",
            "Epoch 186/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.1054\n",
            "Epoch 187/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.1043\n",
            "Epoch 188/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.1042\n",
            "Epoch 189/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.1040\n",
            "Epoch 190/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.1041\n",
            "Epoch 191/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.1054\n",
            "Epoch 192/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.1018\n",
            "Epoch 193/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.1025\n",
            "Epoch 194/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.1014\n",
            "Epoch 195/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.1012\n",
            "Epoch 196/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0974\n",
            "Epoch 197/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.1006\n",
            "Epoch 198/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0986\n",
            "Epoch 199/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0993\n",
            "Epoch 200/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0978\n",
            "Epoch 201/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0937\n",
            "Epoch 202/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.0962\n",
            "Epoch 203/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0965\n",
            "Epoch 204/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0987\n",
            "Epoch 205/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.0964\n",
            "Epoch 206/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0947\n",
            "Epoch 207/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0941\n",
            "Epoch 208/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0927\n",
            "Epoch 209/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0949\n",
            "Epoch 210/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.0927\n",
            "Epoch 211/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0912\n",
            "Epoch 212/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0932\n",
            "Epoch 213/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0929\n",
            "Epoch 214/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0929\n",
            "Epoch 215/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0915\n",
            "Epoch 216/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0916\n",
            "Epoch 217/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0897\n",
            "Epoch 218/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0884\n",
            "Epoch 219/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0891\n",
            "Epoch 220/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0900\n",
            "Epoch 221/1000\n",
            "6/6 [==============================] - 1s 85ms/step - loss: 0.0892\n",
            "Epoch 222/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0916\n",
            "Epoch 223/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0927\n",
            "Epoch 224/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0906\n",
            "Epoch 225/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0886\n",
            "Epoch 226/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.0865\n",
            "Epoch 227/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0848\n",
            "Epoch 228/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0841\n",
            "Epoch 229/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0870\n",
            "Epoch 230/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0857\n",
            "Epoch 231/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0828\n",
            "Epoch 232/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0852\n",
            "Epoch 233/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0869\n",
            "Epoch 234/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0840\n",
            "Epoch 235/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0830\n",
            "Epoch 236/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0841\n",
            "Epoch 237/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.0850\n",
            "Epoch 238/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0827\n",
            "Epoch 239/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0843\n",
            "Epoch 240/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0857\n",
            "Epoch 241/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0820\n",
            "Epoch 242/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0822\n",
            "Epoch 243/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0809\n",
            "Epoch 244/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0821\n",
            "Epoch 245/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0792\n",
            "Epoch 246/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0790\n",
            "Epoch 247/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0817\n",
            "Epoch 248/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0783\n",
            "Epoch 249/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0783\n",
            "Epoch 250/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0793\n",
            "Epoch 251/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0803\n",
            "Epoch 252/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0795\n",
            "Epoch 253/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0784\n",
            "Epoch 254/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0787\n",
            "Epoch 255/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0794\n",
            "Epoch 256/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0785\n",
            "Epoch 257/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0789\n",
            "Epoch 258/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0782\n",
            "Epoch 259/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0765\n",
            "Epoch 260/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0793\n",
            "Epoch 261/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0764\n",
            "Epoch 262/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0793\n",
            "Epoch 263/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0764\n",
            "Epoch 264/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0789\n",
            "Epoch 265/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0804\n",
            "Epoch 266/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0767\n",
            "Epoch 267/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0782\n",
            "Epoch 268/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0751\n",
            "Epoch 269/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0767\n",
            "Epoch 270/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0778\n",
            "Epoch 271/1000\n",
            "6/6 [==============================] - 1s 129ms/step - loss: 0.0781\n",
            "Epoch 272/1000\n",
            "6/6 [==============================] - 1s 128ms/step - loss: 0.0771\n",
            "Epoch 273/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0748\n",
            "Epoch 274/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0755\n",
            "Epoch 275/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.0718\n",
            "Epoch 276/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0701\n",
            "Epoch 277/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0719\n",
            "Epoch 278/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0716\n",
            "Epoch 279/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0741\n",
            "Epoch 280/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0710\n",
            "Epoch 281/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0731\n",
            "Epoch 282/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0742\n",
            "Epoch 283/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0739\n",
            "Epoch 284/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0701\n",
            "Epoch 285/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0731\n",
            "Epoch 286/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0719\n",
            "Epoch 287/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0725\n",
            "Epoch 288/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0701\n",
            "Epoch 289/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0710\n",
            "Epoch 290/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0691\n",
            "Epoch 291/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0726\n",
            "Epoch 292/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0704\n",
            "Epoch 293/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0695\n",
            "Epoch 294/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0696\n",
            "Epoch 295/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0706\n",
            "Epoch 296/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0708\n",
            "Epoch 297/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0673\n",
            "Epoch 298/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0678\n",
            "Epoch 299/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0683\n",
            "Epoch 300/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0677\n",
            "Epoch 301/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0686\n",
            "Epoch 302/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0691\n",
            "Epoch 303/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0690\n",
            "Epoch 304/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0699\n",
            "Epoch 305/1000\n",
            "6/6 [==============================] - 1s 86ms/step - loss: 0.0671\n",
            "Epoch 306/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0690\n",
            "Epoch 307/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0692\n",
            "Epoch 308/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0668\n",
            "Epoch 309/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0681\n",
            "Epoch 310/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0684\n",
            "Epoch 311/1000\n",
            "6/6 [==============================] - 1s 166ms/step - loss: 0.0681\n",
            "Epoch 312/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0700\n",
            "Epoch 313/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0684\n",
            "Epoch 314/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0677\n",
            "Epoch 315/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0689\n",
            "Epoch 316/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0678\n",
            "Epoch 317/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0689\n",
            "Epoch 318/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0685\n",
            "Epoch 319/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0676\n",
            "Epoch 320/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0664\n",
            "Epoch 321/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0657\n",
            "Epoch 322/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0692\n",
            "Epoch 323/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0671\n",
            "Epoch 324/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0674\n",
            "Epoch 325/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0680\n",
            "Epoch 326/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0689\n",
            "Epoch 327/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0655\n",
            "Epoch 328/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0680\n",
            "Epoch 329/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0647\n",
            "Epoch 330/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0655\n",
            "Epoch 331/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0662\n",
            "Epoch 332/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0675\n",
            "Epoch 333/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0660\n",
            "Epoch 334/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0653\n",
            "Epoch 335/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0659\n",
            "Epoch 336/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0649\n",
            "Epoch 337/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0655\n",
            "Epoch 338/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0634\n",
            "Epoch 339/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0643\n",
            "Epoch 340/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0628\n",
            "Epoch 341/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0646\n",
            "Epoch 342/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0639\n",
            "Epoch 343/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0623\n",
            "Epoch 344/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0628\n",
            "Epoch 345/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0640\n",
            "Epoch 346/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0622\n",
            "Epoch 347/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0635\n",
            "Epoch 348/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0640\n",
            "Epoch 349/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0614\n",
            "Epoch 350/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0626\n",
            "Epoch 351/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0613\n",
            "Epoch 352/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0600\n",
            "Epoch 353/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0626\n",
            "Epoch 354/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0646\n",
            "Epoch 355/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0652\n",
            "Epoch 356/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0634\n",
            "Epoch 357/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0651\n",
            "Epoch 358/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0623\n",
            "Epoch 359/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0584\n",
            "Epoch 360/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0634\n",
            "Epoch 361/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0621\n",
            "Epoch 362/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0597\n",
            "Epoch 363/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0623\n",
            "Epoch 364/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0610\n",
            "Epoch 365/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0618\n",
            "Epoch 366/1000\n",
            "6/6 [==============================] - 1s 200ms/step - loss: 0.0598\n",
            "Epoch 367/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0622\n",
            "Epoch 368/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0611\n",
            "Epoch 369/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0620\n",
            "Epoch 370/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0601\n",
            "Epoch 371/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0579\n",
            "Epoch 372/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0589\n",
            "Epoch 373/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0605\n",
            "Epoch 374/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0576\n",
            "Epoch 375/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0593\n",
            "Epoch 376/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0608\n",
            "Epoch 377/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0572\n",
            "Epoch 378/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0607\n",
            "Epoch 379/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0586\n",
            "Epoch 380/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0585\n",
            "Epoch 381/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0579\n",
            "Epoch 382/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0577\n",
            "Epoch 383/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0582\n",
            "Epoch 384/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0594\n",
            "Epoch 385/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0597\n",
            "Epoch 386/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0592\n",
            "Epoch 387/1000\n",
            "6/6 [==============================] - 1s 178ms/step - loss: 0.0597\n",
            "Epoch 388/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0589\n",
            "Epoch 389/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0574\n",
            "Epoch 390/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0600\n",
            "Epoch 391/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0601\n",
            "Epoch 392/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0570\n",
            "Epoch 393/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0570\n",
            "Epoch 394/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0589\n",
            "Epoch 395/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0586\n",
            "Epoch 396/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0588\n",
            "Epoch 397/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0562\n",
            "Epoch 398/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0571\n",
            "Epoch 399/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0565\n",
            "Epoch 400/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0580\n",
            "Epoch 401/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0571\n",
            "Epoch 402/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0565\n",
            "Epoch 403/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0573\n",
            "Epoch 404/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0578\n",
            "Epoch 405/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0567\n",
            "Epoch 406/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0570\n",
            "Epoch 407/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0541\n",
            "Epoch 408/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0556\n",
            "Epoch 409/1000\n",
            "6/6 [==============================] - 1s 198ms/step - loss: 0.0552\n",
            "Epoch 410/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0566\n",
            "Epoch 411/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0570\n",
            "Epoch 412/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0544\n",
            "Epoch 413/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0565\n",
            "Epoch 414/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0560\n",
            "Epoch 415/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0577\n",
            "Epoch 416/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0546\n",
            "Epoch 417/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0567\n",
            "Epoch 418/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0556\n",
            "Epoch 419/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0569\n",
            "Epoch 420/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0570\n",
            "Epoch 421/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0570\n",
            "Epoch 422/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0559\n",
            "Epoch 423/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0571\n",
            "Epoch 424/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0567\n",
            "Epoch 425/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0567\n",
            "Epoch 426/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0532\n",
            "Epoch 427/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0560\n",
            "Epoch 428/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0555\n",
            "Epoch 429/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0541\n",
            "Epoch 430/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0524\n",
            "Epoch 431/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0551\n",
            "Epoch 432/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0549\n",
            "Epoch 433/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0549\n",
            "Epoch 434/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0549\n",
            "Epoch 435/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0547\n",
            "Epoch 436/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0568\n",
            "Epoch 437/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0570\n",
            "Epoch 438/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0548\n",
            "Epoch 439/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0541\n",
            "Epoch 440/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0560\n",
            "Epoch 441/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0598\n",
            "Epoch 442/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0610\n",
            "Epoch 443/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0661\n",
            "Epoch 444/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0654\n",
            "Epoch 445/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0665\n",
            "Epoch 446/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0666\n",
            "Epoch 447/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0649\n",
            "Epoch 448/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0642\n",
            "Epoch 449/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0649\n",
            "Epoch 450/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0624\n",
            "Epoch 451/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0650\n",
            "Epoch 452/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0630\n",
            "Epoch 453/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0618\n",
            "Epoch 454/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0627\n",
            "Epoch 455/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0602\n",
            "Epoch 456/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0582\n",
            "Epoch 457/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0571\n",
            "Epoch 458/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0568\n",
            "Epoch 459/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0550\n",
            "Epoch 460/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0565\n",
            "Epoch 461/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0548\n",
            "Epoch 462/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0561\n",
            "Epoch 463/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0527\n",
            "Epoch 464/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0549\n",
            "Epoch 465/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0537\n",
            "Epoch 466/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0538\n",
            "Epoch 467/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0546\n",
            "Epoch 468/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0550\n",
            "Epoch 469/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0527\n",
            "Epoch 470/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0523\n",
            "Epoch 471/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0526\n",
            "Epoch 472/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0526\n",
            "Epoch 473/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0525\n",
            "Epoch 474/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0530\n",
            "Epoch 475/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0513\n",
            "Epoch 476/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0547\n",
            "Epoch 477/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0540\n",
            "Epoch 478/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0520\n",
            "Epoch 479/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0530\n",
            "Epoch 480/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0523\n",
            "Epoch 481/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0540\n",
            "Epoch 482/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0532\n",
            "Epoch 483/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0530\n",
            "Epoch 484/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0512\n",
            "Epoch 485/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0532\n",
            "Epoch 486/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0532\n",
            "Epoch 487/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0539\n",
            "Epoch 488/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0522\n",
            "Epoch 489/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0519\n",
            "Epoch 490/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0525\n",
            "Epoch 491/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0537\n",
            "Epoch 492/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0521\n",
            "Epoch 493/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0498\n",
            "Epoch 494/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0535\n",
            "Epoch 495/1000\n",
            "6/6 [==============================] - 1s 111ms/step - loss: 0.0532\n",
            "Epoch 496/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0529\n",
            "Epoch 497/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0506\n",
            "Epoch 498/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0511\n",
            "Epoch 499/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0514\n",
            "Epoch 500/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0532\n",
            "Epoch 501/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0485\n",
            "Epoch 502/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0526\n",
            "Epoch 503/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0525\n",
            "Epoch 504/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0510\n",
            "Epoch 505/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0506\n",
            "Epoch 506/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0519\n",
            "Epoch 507/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0513\n",
            "Epoch 508/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0510\n",
            "Epoch 509/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0509\n",
            "Epoch 510/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0505\n",
            "Epoch 511/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0500\n",
            "Epoch 512/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0510\n",
            "Epoch 513/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0501\n",
            "Epoch 514/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0532\n",
            "Epoch 515/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0543\n",
            "Epoch 516/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0503\n",
            "Epoch 517/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0540\n",
            "Epoch 518/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0519\n",
            "Epoch 519/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0537\n",
            "Epoch 520/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0531\n",
            "Epoch 521/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0522\n",
            "Epoch 522/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0513\n",
            "Epoch 523/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0526\n",
            "Epoch 524/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0519\n",
            "Epoch 525/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0500\n",
            "Epoch 526/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0503\n",
            "Epoch 527/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0525\n",
            "Epoch 528/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0518\n",
            "Epoch 529/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0506\n",
            "Epoch 530/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0496\n",
            "Epoch 531/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0495\n",
            "Epoch 532/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0523\n",
            "Epoch 533/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0514\n",
            "Epoch 534/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0510\n",
            "Epoch 535/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0509\n",
            "Epoch 536/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0504\n",
            "Epoch 537/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0507\n",
            "Epoch 538/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0496\n",
            "Epoch 539/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0515\n",
            "Epoch 540/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0525\n",
            "Epoch 541/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0497\n",
            "Epoch 542/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0483\n",
            "Epoch 543/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0499\n",
            "Epoch 544/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0506\n",
            "Epoch 545/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0505\n",
            "Epoch 546/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0514\n",
            "Epoch 547/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0516\n",
            "Epoch 548/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0503\n",
            "Epoch 549/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0507\n",
            "Epoch 550/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0541\n",
            "Epoch 551/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0491\n",
            "Epoch 552/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0489\n",
            "Epoch 553/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0504\n",
            "Epoch 554/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0496\n",
            "Epoch 555/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0501\n",
            "Epoch 556/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0498\n",
            "Epoch 557/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0511\n",
            "Epoch 558/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0506\n",
            "Epoch 559/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0503\n",
            "Epoch 560/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0512\n",
            "Epoch 561/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0499\n",
            "Epoch 562/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0492\n",
            "Epoch 563/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0493\n",
            "Epoch 564/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0501\n",
            "Epoch 565/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0500\n",
            "Epoch 566/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0506\n",
            "Epoch 567/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0515\n",
            "Epoch 568/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0477\n",
            "Epoch 569/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0495\n",
            "Epoch 570/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0478\n",
            "Epoch 571/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0493\n",
            "Epoch 572/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0490\n",
            "Epoch 573/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0490\n",
            "Epoch 574/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0505\n",
            "Epoch 575/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0497\n",
            "Epoch 576/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0517\n",
            "Epoch 577/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0494\n",
            "Epoch 578/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0504\n",
            "Epoch 579/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0508\n",
            "Epoch 580/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0490\n",
            "Epoch 581/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0514\n",
            "Epoch 582/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0529\n",
            "Epoch 583/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0475\n",
            "Epoch 584/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0492\n",
            "Epoch 585/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0482\n",
            "Epoch 586/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0506\n",
            "Epoch 587/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0498\n",
            "Epoch 588/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0493\n",
            "Epoch 589/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0494\n",
            "Epoch 590/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0493\n",
            "Epoch 591/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0485\n",
            "Epoch 592/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0499\n",
            "Epoch 593/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0493\n",
            "Epoch 594/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0489\n",
            "Epoch 595/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0485\n",
            "Epoch 596/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0493\n",
            "Epoch 597/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0479\n",
            "Epoch 598/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0485\n",
            "Epoch 599/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0496\n",
            "Epoch 600/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0488\n",
            "Epoch 601/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0468\n",
            "Epoch 602/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0482\n",
            "Epoch 603/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0486\n",
            "Epoch 604/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0523\n",
            "Epoch 605/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0540\n",
            "Epoch 606/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0548\n",
            "Epoch 607/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0565\n",
            "Epoch 608/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0569\n",
            "Epoch 609/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0607\n",
            "Epoch 610/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0609\n",
            "Epoch 611/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0578\n",
            "Epoch 612/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0569\n",
            "Epoch 613/1000\n",
            "6/6 [==============================] - 1s 114ms/step - loss: 0.0559\n",
            "Epoch 614/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0544\n",
            "Epoch 615/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0548\n",
            "Epoch 616/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0533\n",
            "Epoch 617/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0517\n",
            "Epoch 618/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0504\n",
            "Epoch 619/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0499\n",
            "Epoch 620/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0499\n",
            "Epoch 621/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0500\n",
            "Epoch 622/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0501\n",
            "Epoch 623/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0486\n",
            "Epoch 624/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0520\n",
            "Epoch 625/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0490\n",
            "Epoch 626/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0492\n",
            "Epoch 627/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0487\n",
            "Epoch 628/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0486\n",
            "Epoch 629/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0481\n",
            "Epoch 630/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0475\n",
            "Epoch 631/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0487\n",
            "Epoch 632/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0464\n",
            "Epoch 633/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0488\n",
            "Epoch 634/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0477\n",
            "Epoch 635/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0491\n",
            "Epoch 636/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0484\n",
            "Epoch 637/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0483\n",
            "Epoch 638/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0476\n",
            "Epoch 639/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0500\n",
            "Epoch 640/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0482\n",
            "Epoch 641/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0479\n",
            "Epoch 642/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0486\n",
            "Epoch 643/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0483\n",
            "Epoch 644/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0499\n",
            "Epoch 645/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0493\n",
            "Epoch 646/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0475\n",
            "Epoch 647/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0499\n",
            "Epoch 648/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0507\n",
            "Epoch 649/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0478\n",
            "Epoch 650/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0512\n",
            "Epoch 651/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0485\n",
            "Epoch 652/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0487\n",
            "Epoch 653/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0492\n",
            "Epoch 654/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0477\n",
            "Epoch 655/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0489\n",
            "Epoch 656/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0475\n",
            "Epoch 657/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0464\n",
            "Epoch 658/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0477\n",
            "Epoch 659/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0472\n",
            "Epoch 660/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0474\n",
            "Epoch 661/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0466\n",
            "Epoch 662/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0499\n",
            "Epoch 663/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0469\n",
            "Epoch 664/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0450\n",
            "Epoch 665/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0483\n",
            "Epoch 666/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0464\n",
            "Epoch 667/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0492\n",
            "Epoch 668/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0454\n",
            "Epoch 669/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0480\n",
            "Epoch 670/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0478\n",
            "Epoch 671/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0494\n",
            "Epoch 672/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0468\n",
            "Epoch 673/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0469\n",
            "Epoch 674/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0479\n",
            "Epoch 675/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0445\n",
            "Epoch 676/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0489\n",
            "Epoch 677/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0455\n",
            "Epoch 678/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0467\n",
            "Epoch 679/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0460\n",
            "Epoch 680/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0460\n",
            "Epoch 681/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0469\n",
            "Epoch 682/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0468\n",
            "Epoch 683/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0464\n",
            "Epoch 684/1000\n",
            "6/6 [==============================] - 1s 141ms/step - loss: 0.0479\n",
            "Epoch 685/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0452\n",
            "Epoch 686/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0465\n",
            "Epoch 687/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0474\n",
            "Epoch 688/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0466\n",
            "Epoch 689/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0508\n",
            "Epoch 690/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0488\n",
            "Epoch 691/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0513\n",
            "Epoch 692/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0514\n",
            "Epoch 693/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0505\n",
            "Epoch 694/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0504\n",
            "Epoch 695/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0513\n",
            "Epoch 696/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0492\n",
            "Epoch 697/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0498\n",
            "Epoch 698/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0481\n",
            "Epoch 699/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0477\n",
            "Epoch 700/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0486\n",
            "Epoch 701/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0485\n",
            "Epoch 702/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0472\n",
            "Epoch 703/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0465\n",
            "Epoch 704/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0484\n",
            "Epoch 705/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0461\n",
            "Epoch 706/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0480\n",
            "Epoch 707/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0476\n",
            "Epoch 708/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0481\n",
            "Epoch 709/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0468\n",
            "Epoch 710/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0493\n",
            "Epoch 711/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0492\n",
            "Epoch 712/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0508\n",
            "Epoch 713/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0503\n",
            "Epoch 714/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0474\n",
            "Epoch 715/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0489\n",
            "Epoch 716/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0465\n",
            "Epoch 717/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0486\n",
            "Epoch 718/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0472\n",
            "Epoch 719/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0457\n",
            "Epoch 720/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0467\n",
            "Epoch 721/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0456\n",
            "Epoch 722/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0473\n",
            "Epoch 723/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0462\n",
            "Epoch 724/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0472\n",
            "Epoch 725/1000\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 0.0464\n",
            "Epoch 726/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0473\n",
            "Epoch 727/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0460\n",
            "Epoch 728/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0456\n",
            "Epoch 729/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0474\n",
            "Epoch 730/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0476\n",
            "Epoch 731/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0435\n",
            "Epoch 732/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0466\n",
            "Epoch 733/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0471\n",
            "Epoch 734/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0474\n",
            "Epoch 735/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0473\n",
            "Epoch 736/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0468\n",
            "Epoch 737/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0461\n",
            "Epoch 738/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0447\n",
            "Epoch 739/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0478\n",
            "Epoch 740/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0450\n",
            "Epoch 741/1000\n",
            "6/6 [==============================] - 1s 122ms/step - loss: 0.0457\n",
            "Epoch 742/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0461\n",
            "Epoch 743/1000\n",
            "6/6 [==============================] - 1s 117ms/step - loss: 0.0476\n",
            "Epoch 744/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0459\n",
            "Epoch 745/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0451\n",
            "Epoch 746/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0472\n",
            "Epoch 747/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0472\n",
            "Epoch 748/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0466\n",
            "Epoch 749/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0467\n",
            "Epoch 750/1000\n",
            "6/6 [==============================] - 1s 112ms/step - loss: 0.0460\n",
            "Epoch 751/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0473\n",
            "Epoch 752/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0455\n",
            "Epoch 753/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0449\n",
            "Epoch 754/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0455\n",
            "Epoch 755/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0454\n",
            "Epoch 756/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0469\n",
            "Epoch 757/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0468\n",
            "Epoch 758/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0459\n",
            "Epoch 759/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0459\n",
            "Epoch 760/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0449\n",
            "Epoch 761/1000\n",
            "6/6 [==============================] - 1s 111ms/step - loss: 0.0480\n",
            "Epoch 762/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0454\n",
            "Epoch 763/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0450\n",
            "Epoch 764/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0446\n",
            "Epoch 765/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0461\n",
            "Epoch 766/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0467\n",
            "Epoch 767/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0452\n",
            "Epoch 768/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0461\n",
            "Epoch 769/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0465\n",
            "Epoch 770/1000\n",
            "6/6 [==============================] - 1s 112ms/step - loss: 0.0471\n",
            "Epoch 771/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0514\n",
            "Epoch 772/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0545\n",
            "Epoch 773/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0591\n",
            "Epoch 774/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0711\n",
            "Epoch 775/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0822\n",
            "Epoch 776/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0981\n",
            "Epoch 777/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.1128\n",
            "Epoch 778/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.1139\n",
            "Epoch 779/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.1084\n",
            "Epoch 780/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0975\n",
            "Epoch 781/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0825\n",
            "Epoch 782/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0741\n",
            "Epoch 783/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0664\n",
            "Epoch 784/1000\n",
            "6/6 [==============================] - 1s 136ms/step - loss: 0.0625\n",
            "Epoch 785/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0605\n",
            "Epoch 786/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0592\n",
            "Epoch 787/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0544\n",
            "Epoch 788/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0525\n",
            "Epoch 789/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0533\n",
            "Epoch 790/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0543\n",
            "Epoch 791/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0536\n",
            "Epoch 792/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0524\n",
            "Epoch 793/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0535\n",
            "Epoch 794/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0499\n",
            "Epoch 795/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0518\n",
            "Epoch 796/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0525\n",
            "Epoch 797/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0499\n",
            "Epoch 798/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0507\n",
            "Epoch 799/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0501\n",
            "Epoch 800/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0476\n",
            "Epoch 801/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0492\n",
            "Epoch 802/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0483\n",
            "Epoch 803/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0488\n",
            "Epoch 804/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0489\n",
            "Epoch 805/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0486\n",
            "Epoch 806/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0503\n",
            "Epoch 807/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0497\n",
            "Epoch 808/1000\n",
            "6/6 [==============================] - 1s 111ms/step - loss: 0.0484\n",
            "Epoch 809/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0474\n",
            "Epoch 810/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0479\n",
            "Epoch 811/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0488\n",
            "Epoch 812/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0463\n",
            "Epoch 813/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0476\n",
            "Epoch 814/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0471\n",
            "Epoch 815/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0453\n",
            "Epoch 816/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0464\n",
            "Epoch 817/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0467\n",
            "Epoch 818/1000\n",
            "6/6 [==============================] - 1s 131ms/step - loss: 0.0478\n",
            "Epoch 819/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0462\n",
            "Epoch 820/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0469\n",
            "Epoch 821/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0462\n",
            "Epoch 822/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0476\n",
            "Epoch 823/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0464\n",
            "Epoch 824/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0468\n",
            "Epoch 825/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0471\n",
            "Epoch 826/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0444\n",
            "Epoch 827/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0450\n",
            "Epoch 828/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0460\n",
            "Epoch 829/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0451\n",
            "Epoch 830/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0460\n",
            "Epoch 831/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0453\n",
            "Epoch 832/1000\n",
            "6/6 [==============================] - 1s 159ms/step - loss: 0.0448\n",
            "Epoch 833/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0476\n",
            "Epoch 834/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0468\n",
            "Epoch 835/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0455\n",
            "Epoch 836/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0459\n",
            "Epoch 837/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0462\n",
            "Epoch 838/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0450\n",
            "Epoch 839/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0458\n",
            "Epoch 840/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0458\n",
            "Epoch 841/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0445\n",
            "Epoch 842/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0444\n",
            "Epoch 843/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0443\n",
            "Epoch 844/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0446\n",
            "Epoch 845/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0458\n",
            "Epoch 846/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0439\n",
            "Epoch 847/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0460\n",
            "Epoch 848/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0455\n",
            "Epoch 849/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0451\n",
            "Epoch 850/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0433\n",
            "Epoch 851/1000\n",
            "6/6 [==============================] - 1s 87ms/step - loss: 0.0465\n",
            "Epoch 852/1000\n",
            "6/6 [==============================] - 1s 97ms/step - loss: 0.0441\n",
            "Epoch 853/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0445\n",
            "Epoch 854/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0449\n",
            "Epoch 855/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0443\n",
            "Epoch 856/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0462\n",
            "Epoch 857/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0444\n",
            "Epoch 858/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0453\n",
            "Epoch 859/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0447\n",
            "Epoch 860/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0446\n",
            "Epoch 861/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0430\n",
            "Epoch 862/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0445\n",
            "Epoch 863/1000\n",
            "6/6 [==============================] - 1s 112ms/step - loss: 0.0453\n",
            "Epoch 864/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0425\n",
            "Epoch 865/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0462\n",
            "Epoch 866/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0461\n",
            "Epoch 867/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0441\n",
            "Epoch 868/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0449\n",
            "Epoch 869/1000\n",
            "6/6 [==============================] - 1s 95ms/step - loss: 0.0432\n",
            "Epoch 870/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0432\n",
            "Epoch 871/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0445\n",
            "Epoch 872/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0444\n",
            "Epoch 873/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0444\n",
            "Epoch 874/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0437\n",
            "Epoch 875/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0449\n",
            "Epoch 876/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0451\n",
            "Epoch 877/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0446\n",
            "Epoch 878/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0431\n",
            "Epoch 879/1000\n",
            "6/6 [==============================] - 1s 96ms/step - loss: 0.0461\n",
            "Epoch 880/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0437\n",
            "Epoch 881/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0446\n",
            "Epoch 882/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0426\n",
            "Epoch 883/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0428\n",
            "Epoch 884/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0467\n",
            "Epoch 885/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0442\n",
            "Epoch 886/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0452\n",
            "Epoch 887/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0439\n",
            "Epoch 888/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0453\n",
            "Epoch 889/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0436\n",
            "Epoch 890/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0448\n",
            "Epoch 891/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0451\n",
            "Epoch 892/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0436\n",
            "Epoch 893/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0442\n",
            "Epoch 894/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0451\n",
            "Epoch 895/1000\n",
            "6/6 [==============================] - 1s 103ms/step - loss: 0.0434\n",
            "Epoch 896/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0455\n",
            "Epoch 897/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0446\n",
            "Epoch 898/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0461\n",
            "Epoch 899/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0450\n",
            "Epoch 900/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0450\n",
            "Epoch 901/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0456\n",
            "Epoch 902/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0442\n",
            "Epoch 903/1000\n",
            "6/6 [==============================] - 1s 112ms/step - loss: 0.0439\n",
            "Epoch 904/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0451\n",
            "Epoch 905/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0448\n",
            "Epoch 906/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0454\n",
            "Epoch 907/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0464\n",
            "Epoch 908/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0450\n",
            "Epoch 909/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0451\n",
            "Epoch 910/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0480\n",
            "Epoch 911/1000\n",
            "6/6 [==============================] - 1s 114ms/step - loss: 0.0448\n",
            "Epoch 912/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0446\n",
            "Epoch 913/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0430\n",
            "Epoch 914/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0446\n",
            "Epoch 915/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0449\n",
            "Epoch 916/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0438\n",
            "Epoch 917/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0453\n",
            "Epoch 918/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0449\n",
            "Epoch 919/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0456\n",
            "Epoch 920/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0448\n",
            "Epoch 921/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0439\n",
            "Epoch 922/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0441\n",
            "Epoch 923/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0438\n",
            "Epoch 924/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0442\n",
            "Epoch 925/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0429\n",
            "Epoch 926/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0443\n",
            "Epoch 927/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0432\n",
            "Epoch 928/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0451\n",
            "Epoch 929/1000\n",
            "6/6 [==============================] - 1s 102ms/step - loss: 0.0427\n",
            "Epoch 930/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0428\n",
            "Epoch 931/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0446\n",
            "Epoch 932/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0441\n",
            "Epoch 933/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0444\n",
            "Epoch 934/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0444\n",
            "Epoch 935/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0429\n",
            "Epoch 936/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0429\n",
            "Epoch 937/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0439\n",
            "Epoch 938/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0453\n",
            "Epoch 939/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0436\n",
            "Epoch 940/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0437\n",
            "Epoch 941/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0432\n",
            "Epoch 942/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0431\n",
            "Epoch 943/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0431\n",
            "Epoch 944/1000\n",
            "6/6 [==============================] - 1s 111ms/step - loss: 0.0438\n",
            "Epoch 945/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0437\n",
            "Epoch 946/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0445\n",
            "Epoch 947/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0449\n",
            "Epoch 948/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0455\n",
            "Epoch 949/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0438\n",
            "Epoch 950/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0439\n",
            "Epoch 951/1000\n",
            "6/6 [==============================] - 1s 101ms/step - loss: 0.0442\n",
            "Epoch 952/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0445\n",
            "Epoch 953/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0431\n",
            "Epoch 954/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0438\n",
            "Epoch 955/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0446\n",
            "Epoch 956/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0458\n",
            "Epoch 957/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0446\n",
            "Epoch 958/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0443\n",
            "Epoch 959/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0462\n",
            "Epoch 960/1000\n",
            "6/6 [==============================] - 1s 109ms/step - loss: 0.0461\n",
            "Epoch 961/1000\n",
            "6/6 [==============================] - 1s 89ms/step - loss: 0.0467\n",
            "Epoch 962/1000\n",
            "6/6 [==============================] - 1s 105ms/step - loss: 0.0492\n",
            "Epoch 963/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0464\n",
            "Epoch 964/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0490\n",
            "Epoch 965/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0472\n",
            "Epoch 966/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0471\n",
            "Epoch 967/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0459\n",
            "Epoch 968/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0465\n",
            "Epoch 969/1000\n",
            "6/6 [==============================] - 1s 98ms/step - loss: 0.0449\n",
            "Epoch 970/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0442\n",
            "Epoch 971/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0445\n",
            "Epoch 972/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0450\n",
            "Epoch 973/1000\n",
            "6/6 [==============================] - 1s 111ms/step - loss: 0.0429\n",
            "Epoch 974/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0451\n",
            "Epoch 975/1000\n",
            "6/6 [==============================] - 1s 108ms/step - loss: 0.0447\n",
            "Epoch 976/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0418\n",
            "Epoch 977/1000\n",
            "6/6 [==============================] - 1s 110ms/step - loss: 0.0423\n",
            "Epoch 978/1000\n",
            "6/6 [==============================] - 1s 88ms/step - loss: 0.0414\n",
            "Epoch 979/1000\n",
            "6/6 [==============================] - 1s 111ms/step - loss: 0.0442\n",
            "Epoch 980/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0455\n",
            "Epoch 981/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0441\n",
            "Epoch 982/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0447\n",
            "Epoch 983/1000\n",
            "6/6 [==============================] - 1s 117ms/step - loss: 0.0440\n",
            "Epoch 984/1000\n",
            "6/6 [==============================] - 1s 91ms/step - loss: 0.0439\n",
            "Epoch 985/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0460\n",
            "Epoch 986/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0425\n",
            "Epoch 987/1000\n",
            "6/6 [==============================] - 1s 107ms/step - loss: 0.0451\n",
            "Epoch 988/1000\n",
            "6/6 [==============================] - 1s 170ms/step - loss: 0.0447\n",
            "Epoch 989/1000\n",
            "6/6 [==============================] - 1s 94ms/step - loss: 0.0449\n",
            "Epoch 990/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0441\n",
            "Epoch 991/1000\n",
            "6/6 [==============================] - 1s 90ms/step - loss: 0.0430\n",
            "Epoch 992/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0446\n",
            "Epoch 993/1000\n",
            "6/6 [==============================] - 1s 93ms/step - loss: 0.0425\n",
            "Epoch 994/1000\n",
            "6/6 [==============================] - 1s 125ms/step - loss: 0.0439\n",
            "Epoch 995/1000\n",
            "6/6 [==============================] - 1s 99ms/step - loss: 0.0430\n",
            "Epoch 996/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0433\n",
            "Epoch 997/1000\n",
            "6/6 [==============================] - 1s 106ms/step - loss: 0.0432\n",
            "Epoch 998/1000\n",
            "6/6 [==============================] - 1s 92ms/step - loss: 0.0432\n",
            "Epoch 999/1000\n",
            "6/6 [==============================] - 1s 104ms/step - loss: 0.0429\n",
            "Epoch 1000/1000\n",
            "6/6 [==============================] - 1s 100ms/step - loss: 0.0437\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "  x=email_dataset,\n",
        "  epochs=EPOCHS,\n",
        "  callbacks=[\n",
        "    checkpoint_callback\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLdnOyvzMggJ"
      },
      "outputs": [],
      "source": [
        "def render_training_history(training_history):\n",
        "    loss = training_history.history['loss']\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(loss, label='Training set')\n",
        "    plt.legend()\n",
        "    plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ghveem_OQBV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "dfa27790-2562-4285-8326-03ec29e3cea2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "render_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-dhNP2OG2EM"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU_YfP6sG3NC"
      },
      "source": [
        "### Restore the latest checkpoint\n",
        "\n",
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
        "\n",
        "To run the model with a different `batch_size`, we need to rebuild the model and restore the weights from the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlG8o3wiG6f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8f91e288-012e-4399-c7a2-565fd992ac4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tmp/checkpoints/ckpt_1000'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7evN0LvH01P"
      },
      "outputs": [],
      "source": [
        "simplified_batch_size = 1\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([simplified_batch_size, None]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3eduDtZI9zQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d43e2cb6-e415-4d34-e5d3-e658b11653b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (1, None, 256)            23296     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (1, None, 1024)           5246976   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 91)             93275     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,363,547\n",
            "Trainable params: 5,363,547\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euNvAtr4JC3A"
      },
      "source": [
        "### The prediction loop\n",
        "\n",
        "The following code block generates the text:\n",
        "\n",
        "- It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "\n",
        "- Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "\n",
        "- Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "\n",
        "- The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n",
        "\n",
        "![Prediction loop](https://www.tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n",
        "\n",
        "Image source: [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation) notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOqdqGouJFf_"
      },
      "outputs": [],
      "source": [
        "# num_generate\n",
        "# - number of characters to generate.\n",
        "#\n",
        "# temperature\n",
        "# - Low temperatures results in more predictable text.\n",
        "# - Higher temperatures results in more surprising text.\n",
        "# - Experiment to find the best setting.\n",
        "def generate_text(model, start_string, num_generate = 1000, temperature=1.0):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing).\n",
        "    input_indices = [letter_char2index[s] for s in start_string]\n",
        "    input_indices = tf.expand_dims(input_indices, 0)\n",
        "\n",
        "    # Empty string to store our results.\n",
        "    text_generated = []\n",
        "\n",
        "    # Here batch size == 1.\n",
        "    model.reset_states()\n",
        "    for char_index in range(num_generate):\n",
        "        predictions = model(input_indices)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Using a categorical distribution to predict the character returned by the model.\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(\n",
        "        predictions,\n",
        "        num_samples=1\n",
        "        )[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state.\n",
        "        input_indices = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(letter_index2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-8e8P60J5Pg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc3a7897-1340-40cf-9130-fe31defa3910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Friday, April 29th, 2022, night\n",
            "uibu\n",
            "Hâ€¦wf\n",
            "tqfou\n",
            "b\n",
            "mpu\n",
            "pg\n",
            "ujnf\n",
            "uijoljoh\n",
            "bcpvu\n",
            "boe\n",
            "uibu\n",
            "uifz\n",
            "epoâ€¦u\n",
            "lopx,\n",
            "H\n",
            "epoâ€¦u\n",
            "uijol\n",
            "uifz\n",
            "voefstuboe\n",
            "nf\n",
            "bcmf\n",
            "up\n",
            "qsfqbsf\n",
            "gps\n",
            "ujnf\n",
            "xfmu,\n",
            "Avu\n",
            "bu\n",
            "uif\n",
            "tbnf\n",
            "ujnf*\n",
            "epjoh\n",
            "tp\n",
            "xpvme\n",
            "ibwf\n",
            "bggfdufe\n",
            "nz\n",
            "tmffq\n",
            "boe\n",
            "tdippm\n",
            "uibu\n",
            "ebz\n",
            "tp\n",
            "H\n",
            "dbo\n",
            "voefstuboe\n",
            "xiz\n",
            "H\n",
            "ejeo%u\n",
            "ep\n",
            "ju,\n",
            "H\n",
            "bmtp\n",
            "uijol\n",
            "bcpvu\n",
            "xibu\n",
            "ju\n",
            "xpvme\n",
            "ibwf\n",
            "cffo\n",
            "mjlf\n",
            "up\n",
            "cf\n",
            "bcmf\n",
            "up\n",
            "bqqspbdi\n",
            "ijn\n",
            "jt\n",
            "sfbdijoh\n",
            "pvu\n",
            "up\n",
            "ijn*\n",
            "boe\n",
            "ju\n",
            "nblft\n",
            "nf\n",
            "gffm\n",
            "jotfdvsf,\n",
            "H\n",
            "lffq\n",
            "sfbdijoh\n",
            "pvu\n",
            "up\n",
            "ijn\n",
            "xjuipvu\n",
            "lopxjoh\n",
            "xibu\n",
            "uif\n",
            "gjofmu\n",
            "bodftupst,\n",
            "Afdpnf\n",
            "uifjs\n",
            "mjwjoh\n",
            "nfnpsjbm,\t\tLpoebz*\n",
            "Ibovbsz\n",
            "02ui*\n",
            "0.00*\n",
            "qbtu\n",
            "njeojhiu\tVpx\n",
            "juâ€¦t\n",
            "cffo\n",
            "b\n",
            "xpsl\n",
            "pvu\n",
            "b\n",
            "efbemjof\n",
            "bgufs\n",
            "Svftebz-sfdfjwf\n",
            "bo\n",
            "jodpnqmfuf;\n",
            "\tSijt\n",
            "tjuvbujpo\n",
            "ibt\n",
            "bggfdufe\n",
            "nz\n",
            "nfoubm\n",
            "ifbmujpo\n",
            "H\n",
            "hvfttftt\n",
            "pg\n",
            "uif\n",
            "Ambdl\n",
            "Afmu\n",
            "fyijcjujpo\n",
            "dpmmfdujwfmz\n",
            "bdlopxmfehf\n",
            "boe\n",
            "jogpsn\n",
            "uif\n",
            "xjef+sbohjoh\n",
            "jogmvfodf\n",
            "pg\n",
            "Asvdf\n",
            "Kff\n",
            "bcpvu\n",
            "dvssfoumz\n",
            "uszjoh\n",
            "up\n",
            "hfu\n",
            "jo\n",
            "upvdi\n",
            "xjui\n",
            "Ospgfttps\n",
            "Aspplft\n",
            "bcpvu\n",
            "tpnf\n",
            "qfstpobm\n",
            "djsdvntubodft\n",
            "boe\n",
            "ipx\n",
            "uifz\n",
            "bggfdu\n",
            "nz\n",
            "bcjmjuz\n",
            "up\n",
            "qsfqbsf\n",
            "gps\n",
            "boe\n",
            "ublf\n",
            "gjobm\n",
            "fybnt\n",
            "gps\n",
            "dmbttft\n",
            "/3+/3.\n",
            "boe\n",
            "0/+02/,\tLz\n",
            "qspgfttps\n",
            "gps\n",
            "0/+02/\n",
            "jt\n",
            "wf\n",
            "uibu,\n",
            "?ozxb\n"
          ]
        }
      ],
      "source": [
        "# Generate the text with default temperature (1.0).\n",
        "print(generate_text(model, start_string=u\"Friday, April 29th, 2022, night\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq_NlwWJSdix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c4f3365-6587-4b05-dc52-ba6a3cb495b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dear Lauren,\n",
            "Vifo\n",
            "tif\n",
            "ljttfe\n",
            "nz\n",
            "ofdl\n",
            "H\n",
            "ejeoâ€¦u\n",
            "sfdjqspdbuf\n",
            "Kff\n",
            "bt\n",
            "b\n",
            "tvnnf\n",
            "xp\n",
            "nvdipst\t+\n",
            "?\n",
            "mjuumf\n",
            "bxlxbse\tCvsjodfuEpn\n",
            "ijn\n",
            "zfu\n",
            "tbqqbmz\n",
            "sbqqmfe\n",
            "xjui\n",
            "dpogmjdujoh\n",
            "gffmjoht\n",
            "bcGj\n",
            "Ospgfttps\n",
            "Ng\n",
            "ofnbsjuin\n",
            "eftjsf\n",
            "fbdi\n",
            "xfct\t+\n",
            "?\n",
            "mjujnz\n",
            "gjof\t\tRbuvsebz*\n",
            "?qsjm\n",
            "/4ui*\n",
            "0.00\t?t\n",
            "uif\n",
            "ebzt\n",
            "hfu\n",
            "mpohfs\n",
            "boe\n",
            "tmpxfs\n",
            "boe\n",
            "uif\n",
            "bjs\n",
            "hfut\n",
            "ifbwjfs\n",
            "boe\n",
            "tujdljoh,\n",
            "Vpvme\n",
            "uijt\n",
            "cf\n",
            "qpt\n",
            "cfuxffo\n",
            "qspdfttf,\n",
            "Rif\n",
            "ljotqbdf\n",
            "ifsf*\n",
            "jotjdv,\tKbvsfo\n",
            "Jvwf\tKbvsfo\t\tSvftebz*\n",
            "Cfdfncfs\n",
            "/2ui*\n",
            "0.0/*\n",
            "28.0\n",
            "OL\tGj*\tWftufsebz\n",
            "Bbtfz\n",
            "boe\n",
            "H\n",
            "ejtdvttfe\n",
            "nz\n",
            "ofyu\n",
            "bqqpjounfou\n",
            "xjmm\n",
            "cf\n",
            "Ibovbsz\n",
            "/.ui',\tSibol\n",
            "zpv \tKbvsfo\t\tVfeottebzt\n",
            "zf \tLbz\n",
            "H\n",
            "snvdi\n",
            "up\n",
            "ubml\n",
            "up\n",
            "Bbtfz\n",
            "bcpvu,\n",
            "Rp\n",
            "npsfu\n",
            "xfotujmm\n",
            "bcmf\n",
            "up\n",
            "gpdvt\n",
            "xjui\n",
            "uifn,\n",
            "H\n",
            "qsfuuz\n",
            "nvdi\n",
            "pomz\n",
            "ubmlfe\n",
            "up\n",
            "?mfy\n",
            "gps\n",
            "uif\n",
            "/tu\n",
            "ujnf\n",
            "bgufs\n",
            "uif\n",
            "csfbl\n",
            "vq,\n",
            "Vifo\n",
            "uif\n",
            "jttvnf\n",
            "uijt\n",
            "H\n",
            "ibe\n",
            "tp\n",
            "nvdi\n",
            "xftujft\n",
            "tipvme\n",
            "cfu\n",
            "nf\n",
            "bmui\n",
            "vtu\n",
            "b\n",
            "gfx\n",
            "opeft\n",
            "boe\n",
            "ejggjdvmu,\tH\n",
            "tqplf\n",
            "up\n",
            "nz\n",
            "dipsmpx\n",
            "Cf\tH\n",
            "Kff\n",
            "xijo\n",
            "sfbm\n",
            "mjgf'\n",
            "boe\n",
            "eftubcjmjÃ©ft\n",
            "nbsujbm\n",
            "bsut\n",
            "jo\n",
            "gjmftt\n",
            "pg\n",
            "uif\n",
            "Ambdl\n",
            "Afmu\n",
            "ffstf\t+zpv;\n",
            "8.\t\tSivstetjbo\n",
            "xffmjoh\n",
            "uf\n",
            "Kbt\n",
            "boejmbujpo\n",
            "gjufqumf\n",
            "pg\n",
            "#0\n",
            "boe\n",
            "#1',\t\tH\n",
            "mjlf8\t+uibujpo\n",
            "xjui\n",
            "nz\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate the text with higher temperature to get more unexpected results.\n",
        "print(generate_text(model, start_string=u\"Friday, April 29th, 2022, night\", temperature=1.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ueSZWnyf-jD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "874dbfe8-b468-4d0f-fed7-ae623065ba05"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-9cdb146fe5f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcreate_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mold_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_Model' is not defined"
          ]
        }
      ],
      "source": [
        "# model1= create_Model()\n",
        "# model1.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "# model1.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "# old_weights = model1.get_weights()\n",
        "\n",
        "# for _ in range(10):\n",
        "#     model1.set_weights(old_weights)\n",
        "#     for j in range(0, image_size):\n",
        "#           model1.fit(sample[j], sample_lbl[j])\n",
        "#           prediction= model1.predict(sample[j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTNlXKUhprMC"
      },
      "outputs": [],
      "source": [
        "# model.fit(\n",
        "#   x=email_dataset,\n",
        "#   epochs=50,\n",
        "#   callbacks=[\n",
        "#     checkpoint_callback\n",
        "#   ]\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hh80MqEO_XI"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPE98xa8PA-u"
      },
      "outputs": [],
      "source": [
        "model_name = 'text_generation_shakespeare_rnn.h5'\n",
        "model.save(model_name, save_format='h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYP08xbbTNKp"
      },
      "source": [
        "## Converting the model to web-format\n",
        "\n",
        "To use this model on the web we need to convert it into the format that will be understandable by [tensorflowjs](https://www.tensorflow.org/js). To do so we may use [tfjs-converter](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter) as following:\n",
        "\n",
        "```\n",
        "tensorflowjs_converter --input_format keras \\\n",
        "  ./experiments/text_generation_shakespeare_rnn/text_generation_shakespeare_rnn.h5 \\\n",
        "  ./demos/public/models/text_generation_shakespeare_rnn\n",
        "```\n",
        "\n",
        "You find this experiment in the [Demo app](https://trekhleb.github.io/machine-learning-experiments) and play around with it right in you browser to see how the model performs in real life."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "florence_h_text_generation_rnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}